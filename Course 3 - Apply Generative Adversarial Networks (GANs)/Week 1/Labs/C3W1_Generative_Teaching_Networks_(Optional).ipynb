{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C3W1: Generative Teaching Networks (Optional).ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agrWjxVftMNN"
      },
      "source": [
        "# Generative Teaching Networks (GTN)\n",
        "\n",
        "*Please note that this is an optional notebook, meant to introduce more advanced concepts if you're up for a challenge, so don't worry if you don't completely follow! The first author of this work, Felipe Such, reviewed this notebook for you.*\n",
        "\n",
        "### Goals\n",
        "\n",
        "In this notebook, you'll be implementing a Generative Teaching Network (GTN), first introduced in [Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data](https://arxiv.org/abs/1912.07768) (Such et al. 2019). Essentially, a GTN is composed of a generator (i.e. teacher), which produces synthetic data, and a student, which is trained on this data for some task. The key difference between GTNs and GANs is that GTN models work cooperatively (as opposed to adversarially).\n",
        "\n",
        "Throughout this notebook, you'll gain (deeper) exposure to the following concepts:\n",
        "\n",
        "1. **End-to-End Data Augmentation.** Data augmentation refers to the generation of more data from existing data to *augment* the training set. Examples of this with images include operations like random cropping and flipping. In this sense, the generator performs data augmentation by synthesizing data as extra training data. GTNs differ from previous data augmentation approaches in that:\n",
        "    - The generator and student are trained together, as opposed to training and freezing the generator, then training the student.\n",
        "    - The real data plays a small role: it's only used once every several student updates to update the generator with respect to the student's performance.\n",
        "    - The generated data doesn't look realistic (see visualization later in notebook) yet it's more effective for training the student than real data is!\n",
        "\n",
        "2. **Curriculum Learning.** The generator not only can synthesize data from random noise, but also can learn this random noise, or *curriculum*. By backpropagating through the inputs, the generator can be trained to select the curricula that it deems will be best for student learning.\n",
        "\n",
        "3. **Meta-Learning.** Meta-learning refers to \"learning to learn,\" a broad field that optimizes over different learning tasks to find the best way to learn. You're probably an example of a good meta-learner :). A GTN accomplishes this by training the generator to understand how the student learns, demonstrated via curriculum learning.\n",
        "\n",
        "4. **Neural Architecture Search.** But wait, there's still more! The generator doesn't just guide student training, it also can help determine the optimal student architecture (i.e. which layers, network depth). This concept of learning the best architecture is called Neural Architecture Search, or NAS. Pretty convenient, huh!\n",
        "\n",
        "![Figure 1a from the paper](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/gtn_fig1.png?raw=true)\n",
        "*Figure 1(a) from the [GTN paper](https://arxiv.org/pdf/1912.07768.pdf), providing an overview of the method*\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "1. Understand the concepts of teaching networks, meta-learning, and neural architecture search, and how they relate to the objective of data augmentation.\n",
        "2. Implement and train a GTN on MNIST, and observe how a GTN can accelerate training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVBfg9rgtR7h"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "Start by running the following two cells. The first cell imports packages that you'll use and checks whether the package, [Higher](https://github.com/facebookresearch/higher) (by Facebook Research), is installed. Higher allows you to \"unroll\" inner gradient updates. Unrolling inner updates means that instead of computing updates in a loop where previous updates are overwritten (i.e. one step of traditional SGD), each update is stored, which makes it easier to compute and apply gradients to the generator through multiple updates of the student.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lekB0j-YtH3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e59dcdb-432f-444d-a9f4-36fa684feace"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from torch.autograd import grad\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "if 'higher' not in sys.modules:\n",
        "  !pip install higher\n",
        "import higher as higher\n",
        "\n",
        "print(sys.version)\n",
        "print(torch.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting higher\n",
            "  Downloading higher-0.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from higher) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->higher) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->higher)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->higher)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->higher)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->higher)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->higher)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->higher)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->higher)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->higher)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->higher)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->higher)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->higher)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->higher) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->higher)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->higher) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->higher) (1.3.0)\n",
            "Downloading higher-0.2.1-py3-none-any.whl (27 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, higher\n",
            "Successfully installed higher-0.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\n",
            "2.3.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10Wp24Y017h5"
      },
      "source": [
        "# Set random seeds\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set important parameters\n",
        "learning_rate = 1e-2\n",
        "inner_loop_iterations = 32\n",
        "outer_loop_iterations = 5\n",
        "num_classes = 10\n",
        "\n",
        "noise_size = 64     # size of noise or curriculum vector\n",
        "img_size = 28    # width / height of generated image\n",
        "\n",
        "inner_loop_batch_size = 128\n",
        "outer_loop_batch_size = 128\n",
        "\n",
        "mnist_mean = 0.1307         # for normalizing mnist images\n",
        "mnist_std = 0.3081          # for normalizing mnist images\n",
        "\n",
        "imgs_per_row = num_classes"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mebq3wsH4HeJ"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Download the MNIST dataset and organize it into a `torch.utils.data.Dataset` object. Then apply `torchvision.transforms` to convert raw PIL images to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc17E5a6tWWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00251448-8453-4237-91f2-2e0d220c2759"
      },
      "source": [
        "# Initialize MNIST transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((mnist_mean,), (mnist_std,)),\n",
        "])\n",
        "\n",
        "# Create data splits\n",
        "train = datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "train, val = torch.utils.data.random_split(train, [50000, 10000])\n",
        "test = datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
        "print('Created train, val, and test datasets.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:11<00:00, 891570.52it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 55934.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:06<00:00, 241432.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3307957.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Created train, val, and test datasets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEHDEx0A4rg7"
      },
      "source": [
        "### Dataloader\n",
        "\n",
        "Now wrap your dataset class in a `torch.utils.data.DataLoader` class, which will iterate over batches in training. This class increases memory access bandwidth so retrieving images from your dataset won't be a bottleneck in training. MNIST images are small, so the increase in memory retrieval speed should be relatively trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6oWjCknt0-9"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train, batch_size=outer_loop_batch_size, shuffle=True, drop_last=True, num_workers=1, pin_memory=True,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val, batch_size=outer_loop_batch_size, shuffle=True, drop_last=True, num_workers=1, pin_memory=True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test, batch_size=outer_loop_batch_size, shuffle=True, drop_last=True, num_workers=1, pin_memory=True,\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZMid8yMcqo"
      },
      "source": [
        "## MNIST Classification\n",
        "\n",
        "In this next section, you'll implement and train a GTN on MNIST classification. Note that the student model for this task is a classifier. To extend GTNs to other datasets, you also want to check out the weight normalization technique in the paper --- for now on MNIST, you don't need to worry about this. Alright, let's get started with the generator and classifier's model architecture!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NENBXah36vrh"
      },
      "source": [
        "### Generator\n",
        "\n",
        "Let's now build the generator. For this task, the generator will consist of two fully connected blocks (each consisting of a fully connected layer, a leaky ReLU, and a batch normalization layer) and two convolutional blocks (each consisting of a convolutional layer, a batch normalization layer and a leaky ReLU). A tanh layer is applied to this output to center it around `0` with reasonable standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKsF5G1V68sC"
      },
      "source": [
        "class Teacher(nn.Module):\n",
        "    '''\n",
        "    Implements a Teacher module.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        conv1_filters = 64\n",
        "        fc1_size = 1024\n",
        "\n",
        "        self.fc2_filters = 128\n",
        "        self.fc2_width = img_size\n",
        "        fc2_size = self.fc2_filters * self.fc2_width * self.fc2_width\n",
        "\n",
        "        self.fc1 = nn.Linear(noise_size + num_classes, fc1_size)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, 0.1)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(fc1_size, momentum=0.1)\n",
        "\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, 0.1)\n",
        "        self.bn_fc2 = nn.BatchNorm2d(self.fc2_filters, momentum=0.1)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(self.fc2_filters, conv1_filters, 3, 1, padding=3 // 2)\n",
        "        self.bn_conv1 = nn.BatchNorm2d(conv1_filters, momentum=0.1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(conv1_filters, 1, 3, 1, padding=3 // 2)\n",
        "        self.bn_conv2 = nn.BatchNorm2d(1, momentum=0.1)\n",
        "\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.learner_optim_params = nn.Parameter(torch.tensor([0.02, 0.5]), True)\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        '''\n",
        "        Synthesizes a batch of training examples for the learner.\n",
        "        Args:\n",
        "            x (torch.tensor): shape (b, 64)\n",
        "            target (torch.tensor): shape (b, 10)\n",
        "        '''\n",
        "        # Fully connected block 1\n",
        "        x = torch.cat([x, target], dim=1)   # shape (b, 64+10)\n",
        "        x = self.fc1(x)                     # shape (b, 1024)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = self.bn_fc1(x)\n",
        "\n",
        "        # Fully connected block 2\n",
        "        x = self.fc2(x)                     # shape (b, 128*28*28)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "        x = x.view(                         # shape (b, 128, 28, 28)\n",
        "            -1, self.fc2_filters, self.fc2_width, self.fc2_width\n",
        "        )\n",
        "        x = self.bn_fc2(x)\n",
        "\n",
        "        # Convolutional block 1\n",
        "        x = self.conv1(x)                   # shape (b, 64, 28, 28)\n",
        "        x = self.bn_conv1(x)\n",
        "        x = F.leaky_relu(x, 0.1)\n",
        "\n",
        "        # Convolutional block 2\n",
        "        x = self.conv2(x)                   # shape (b, 1, 28,  28)\n",
        "        x = self.bn_conv2(x)\n",
        "\n",
        "        x = (self.tanh(x) + 1 - 2 * mnist_mean) / (2 * mnist_std)\n",
        "        return x, target"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzswqH-69P4"
      },
      "source": [
        "### Classifier\n",
        "\n",
        "Now let's build the student model, a classifier. Be sure to randomize the number of convolutional filters in the first and second convolution layers so the teacher generalizes to other architectures. This is important since it'll help the teacher perform neural architecture search later.\n",
        "\n",
        "For MNIST classification, the classifier consists of two convolutional blocks (each consisting of a convolutional layer, a leaky ReLU, a batch normalization layer and a max pooling). After these layers, the output is flattened and passed through a fully connected layer, a batch normalization layer, and a softmax to generate probabilities per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZqp0lAz7CmR"
      },
      "source": [
        "class Learner(nn.Module):\n",
        "    '''\n",
        "    Implements a Learner module.\n",
        "    '''\n",
        "    def __init__(self, num_conv1=None, num_conv2=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # Randomly select and evaluate convolutional depth\n",
        "        # for evaluation/comparison in neural architecture search\n",
        "        if num_conv1 is None:\n",
        "            conv1_filters = np.random.randint(32, 64)\n",
        "        else:\n",
        "            conv1_filters = num_conv1\n",
        "        if num_conv2 is None:\n",
        "            conv2_filters = np.random.randint(64, 128)\n",
        "        else:\n",
        "            conv2_filters = num_conv2\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, conv1_filters, 3, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(conv1_filters, momentum=0.1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(conv1_filters, conv2_filters, 3, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(conv2_filters, momentum=0.1)\n",
        "\n",
        "        c1_size = (img_size - 3 + 1) // 2\n",
        "        c2_size = (c1_size - 3 + 1) // 2\n",
        "\n",
        "        self.fc = nn.Linear(conv2_filters * c2_size * c2_size, num_classes)\n",
        "        self.bn3 = nn.BatchNorm1d(num_classes, momentum=0.1)\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjMifQZ3pO0e"
      },
      "source": [
        "### Training\n",
        "\n",
        "*Note: can run on CPU but need high RAM version of Colab.*\n",
        "\n",
        "Now let's extend the inner loop implementation to train the outer loop loss function. After training the classifier on the synthetic data every `inner_loop_iterations`, evaluate the classifier on a batch of real data and backpropagate the resulting loss to the generator.\n",
        "\n",
        "After training for about 50 outer loop iterations, your validation accuracy should approach 90%. This means that your teacher is so good that it gets your student to a 90% validation accuracy with 32 iterations. That's pretty cool! Notice how the images don't look much like numbers. These images represent a compressed version of the training images with the most salient information needed for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KLgaZ8Q7EUj"
      },
      "source": [
        "def generate_img(img_tensor):\n",
        "    '''\n",
        "    Function that renders an MNIST image.\n",
        "    '''\n",
        "    return torchvision.transforms.ToPILImage()(1 - ((img_tensor * mnist_std) + mnist_mean))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSL8cTcH55bK"
      },
      "source": [
        "teacher = Teacher()\n",
        "params_to_train = list(teacher.parameters())\n",
        "\n",
        "# If we want to use a curriculum, we initialize the learnable parameters here\n",
        "use_curriculum = True\n",
        "if use_curriculum:\n",
        "    curriculum = nn.Parameter(torch.randn(inner_loop_iterations, inner_loop_batch_size, noise_size), requires_grad=True)\n",
        "    params_to_train += [curriculum]\n",
        "\n",
        "optimizer_teacher = optim.Adam(params_to_train, lr=learning_rate)\n",
        "\n",
        "# For each inner loop iterations, we use the same sequence of labels.\n",
        "# This allows the curriculum vectors to train to stable labels\n",
        "label = torch.tensor([x % num_classes for x in range(inner_loop_batch_size)])\n",
        "\n",
        "# For the inner loop loss, we use cross entropy\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Here we initialize iterators on the train and val datasets\n",
        "train_iterator = iter(train_loader)\n",
        "val_iterator = iter(val_loader)\n",
        "test_iterator = iter(test_loader)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULasVbFGqCUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "342594e5-7166-49f6-df60-38f2da2cad30"
      },
      "source": [
        "for it, real_data in enumerate(train_loader):\n",
        "\n",
        "    teacher.train()\n",
        "    optimizer_teacher.zero_grad()\n",
        "\n",
        "    # We also optimize the learner learning rate and momentum with the\n",
        "    # outer loop updates\n",
        "    learner_lr = teacher.learner_optim_params[0]\n",
        "    learner_momentum = teacher.learner_optim_params[1]\n",
        "\n",
        "    # Here we sample a learner with random number of conv filters\n",
        "    learner = Learner()\n",
        "    inner_optim = optim.SGD(learner.parameters(), lr=learner_lr.item(), momentum=learner_momentum.item())\n",
        "    learner.train()\n",
        "\n",
        "    inner_losses = []\n",
        "    with higher.innerloop_ctx(learner, inner_optim, override={'lr': [learner_lr], 'momentum': [learner_momentum]}) as (flearner, diffopt):\n",
        "        for step in range(inner_loop_iterations):\n",
        "\n",
        "            # Data generation\n",
        "            if use_curriculum:\n",
        "                z_vec = curriculum[step]\n",
        "            else:\n",
        "                z_vec = torch.randn(inner_loop_batch_size, noise_size)\n",
        "\n",
        "            one_hot = F.one_hot(label, num_classes)\n",
        "\n",
        "            # Pass input to teacher to generate synthetic images\n",
        "            teacher_output, teacher_target = teacher(z_vec, one_hot)\n",
        "\n",
        "            # ====== Show intermediate generated images ======\n",
        "            if step == 0:\n",
        "                print('------------------ Outer loop iteration', it + 1, '------------------')\n",
        "                print('Examples 0 - 9 from beginning of inner loop:')\n",
        "                background = Image.new('L', (img_size * imgs_per_row + imgs_per_row + 1, img_size + 2))\n",
        "                for i in range(imgs_per_row): # indexes column\n",
        "                    background.paste(generate_img(teacher_output[i]), (i * 28 + i + 1, 1))\n",
        "                display(background)\n",
        "\n",
        "            if step == (inner_loop_iterations - 1):\n",
        "                print('Examples 0 - 9 from end of inner loop:')\n",
        "                background = Image.new('L', (img_size * imgs_per_row + imgs_per_row + 1, img_size + 2))\n",
        "                for i in range(imgs_per_row): # indexes column\n",
        "                    background.paste(generate_img(teacher_output[i]), (i * 28 + i + 1, 1))\n",
        "                display(background)\n",
        "\n",
        "            # Pass teacher output to the learner\n",
        "            learner_output = flearner(teacher_output)\n",
        "            loss = loss_fn(learner_output, label)\n",
        "            diffopt.step(loss)\n",
        "\n",
        "            inner_losses.append(loss.item())\n",
        "\n",
        "        correct = 0\n",
        "        data, target = real_data\n",
        "        output = flearner(data)\n",
        "        loss = loss_fn(output, target)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        accuracy_train = correct / target.shape[0]\n",
        "\n",
        "        print(\"Inner loop losses:\", inner_losses)\n",
        "        print(\"Train accuracy:\", accuracy_train)\n",
        "\n",
        "        # Compute accuracy on validation set\n",
        "        data, target = next(val_iterator)\n",
        "        print\n",
        "        output = flearner(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct = pred.eq(target.view_as(pred)).sum().item()\n",
        "        accuracy = correct / outer_loop_batch_size\n",
        "        print(\"Val accuracy:\", accuracy)\n",
        "\n",
        "        if (it == outer_loop_iterations - 1):\n",
        "            # Compute accuracy on test set\n",
        "            correct = 0\n",
        "            for i, (data, target) in enumerate(test_loader):\n",
        "                output = flearner(data)\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            accuracy = correct / (outer_loop_batch_size * len(test_loader))\n",
        "            print(\"----------------------------------\")\n",
        "            print(\"Done training...\")\n",
        "            print(\"Final test accuracy:\", accuracy)\n",
        "\n",
        "            # Final inner loop training curve\n",
        "            plt.plot(np.arange(len(inner_losses)), inner_losses)\n",
        "            plt.xlabel(\"Inner loop iteration\")\n",
        "            plt.ylabel(\"Cross entropy loss\")\n",
        "            plt.show()\n",
        "\n",
        "            break\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "    optimizer_teacher.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ Outer loop iteration 1 ------------------\n",
            "Examples 0 - 9 from beginning of inner loop:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=291x30>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBMBPES6uZdm"
      },
      "source": [
        "## Simple MNIST NAS\n",
        "\n",
        "The key idea of this paper is that performance of larger networks on teacher-generated data is a good proxy for performance on real data, allowing you to search over many more architectures with limited compute. In fact, Such et al. state,\n",
        "\n",
        "> We found that to achieve the same predictive power (rank correlation) as achieved with only 128 SGD steps on GTN-generated data, you would instead need 1200 SGD steps on real data.\n",
        "\n",
        "With enough compute, you could train the teacher on large networks sampled from a NAS space, gradually increasing the number of inner loop updates and architectures per outer loop update. This would provide an end-to-end NAS model. However, Such et al. found that this is prohibitively expensive. Thus, the teacher is only trained on small networks with the hope that it'll generalize well to larger, more powerful networks.\n",
        "\n",
        "Now let's implement a simple neural architecture search (NAS) with your GTN. In this search, you'll optimize the number of convolutional filters for the two-layer student network you trained your teacher on earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQn3a53uQVc"
      },
      "source": [
        "num_architectures = 10\n",
        "\n",
        "best_accuracy = 0\n",
        "\n",
        "for i in range(num_architectures):\n",
        "\n",
        "    # Randomly sample architecture\n",
        "    conv1_filters = np.random.randint(1, 64)\n",
        "    conv2_filters = np.random.randint(1, 128)\n",
        "\n",
        "    learner = Learner(conv1_filters, conv2_filters)\n",
        "    inner_optim = optim.SGD(learner.parameters(), lr=learner_lr.item(), momentum=learner_momentum.item())\n",
        "    learner.train()\n",
        "\n",
        "    # For some reason if we don't use higher here, accuracy drops significantly\n",
        "    with higher.innerloop_ctx(learner, inner_optim, override={'lr': [learner_lr], 'momentum': [learner_momentum]}) as (flearner, diffopt):\n",
        "        for step in range(inner_loop_iterations):\n",
        "\n",
        "            # Data generation\n",
        "            if use_curriculum:\n",
        "                z_vec = curriculum[step]\n",
        "            else:\n",
        "                z_vec = torch.randn(inner_loop_batch_size, noise_size)\n",
        "\n",
        "            one_hot = F.one_hot(label, num_classes)\n",
        "\n",
        "            # Pass input to teacher to generate synthetic images\n",
        "            teacher_output, teacher_target = teacher(z_vec, one_hot)\n",
        "\n",
        "            # Pass teacher output to the learner\n",
        "            learner_output = flearner(teacher_output)\n",
        "            loss = loss_fn(learner_output, label)\n",
        "            diffopt.step(loss)\n",
        "\n",
        "        # Compute accuracy on validation set\n",
        "        correct = 0\n",
        "        for val_idx, (data, target) in enumerate(val_loader, 0):\n",
        "            #if (val_idx == val_iterations): break\n",
        "            output = flearner(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        accuracy = correct / (outer_loop_batch_size * len(val_loader))\n",
        "\n",
        "        if (accuracy > best_accuracy):\n",
        "            best_accuracy = accuracy\n",
        "            filter_counts = (conv1_filters, conv2_filters)\n",
        "\n",
        "        print(\"------------------------- Architecture\", i + 1,\" -------------------------\")\n",
        "        print(\"Num conv1 filters:\", conv1_filters, \", Num conv2 filters:\", conv2_filters, \", Val accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "        if (i == num_architectures - 1):\n",
        "            correct = 0\n",
        "            for test_idx, (data, target) in enumerate(test_loader, 0):\n",
        "                #if (test_idx == test_iterations): break\n",
        "                output = flearner(data)\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            accuracy = correct / (outer_loop_batch_size * len(test_loader))\n",
        "            print(\"------------------------- Best architecture -------------------------\")\n",
        "            print(\"Num conv1 filters:\", filter_counts[0], \", Num conv2 filters:\", filter_counts[1], \", Test accuracy:\", accuracy)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWzzqGHUiufO"
      },
      "source": [
        "### Visualization\n",
        "\n",
        "The rows in the grid produced are images from equally-spaced inner loop iterations where the first row corresponds to the first inner loop iteration and the last row corresponds to the last. The columns correspond to classes where the first column is filled with 0's and the last colummn is filled with 9's."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcNX36MTglMe"
      },
      "source": [
        "imgs_per_row = num_classes\n",
        "rows = inner_loop_iterations // 2 * img_size + inner_loop_iterations // 2 + 1\n",
        "cols = imgs_per_row * img_size + imgs_per_row + 1\n",
        "background = Image.new('L', (cols, rows))\n",
        "\n",
        "for step in range(0, inner_loop_iterations, 2): # indexes row\n",
        "    if use_curriculum:\n",
        "        z_vec = curriculum[step]\n",
        "    else:\n",
        "        z_vec = torch.randn(inner_loop_batch_size, noise_size)\n",
        "\n",
        "    one_hot = F.one_hot(label, num_classes)\n",
        "\n",
        "    teacher_output, teacher_target = teacher(z_vec, one_hot)\n",
        "\n",
        "    for i in range(imgs_per_row): # indexes column\n",
        "        background.paste(generate_img(teacher_output[i]), (i * img_size + i + 1, (step // 2) * img_size + (step // 2) + 1))\n",
        "\n",
        "display(background)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgKEU1W4BL17"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now you know how GTN-generated data augmentation can be useful for training classifiers quickly which in turn allows us to more efficiently search for best performing model architectures. You've seen how the whole process can be trained end-to-end, instead of training one model first, then another — you can certainly apply this principle elsewhere as you build your systems, merging different steps of the training process as you let the gradient flow all the way through your system! You've touched on evolving areas of research, such as curriculum learning, meta-learning, and neural architecture search (NAS), which you can also take to your other projects."
      ]
    }
  ]
}